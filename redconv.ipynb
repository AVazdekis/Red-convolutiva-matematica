{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ConvNet:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, kernel_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Inicialización de pesos usando Xavier\n",
    "        self.weights = []\n",
    "        prev_size = input_size\n",
    "        for size in hidden_sizes + [output_size]:\n",
    "            self.weights.append(np.random.randn(prev_size, size) * np.sqrt(2 / prev_size))\n",
    "            prev_size = size\n",
    "\n",
    "        self.biases = [np.zeros((1, size)) for size in hidden_sizes + [output_size]]\n",
    "\n",
    "    def convolutional(self, image):\n",
    "        image_height, image_width = image.shape\n",
    "        kernel_height, kernel_width = self.kernel_size, self.kernel_size\n",
    "\n",
    "        conv_height = image_height - kernel_height + 1\n",
    "        conv_width = image_width - kernel_width + 1\n",
    "\n",
    "        conv_output = np.zeros((conv_height, conv_width))\n",
    "\n",
    "        for i in range(conv_height):\n",
    "            for j in range(conv_width):\n",
    "                conv_output[i, j] = np.sum(image[i:i+kernel_height, j:j+kernel_width] * self.conv_weights) + self.conv_bias\n",
    "\n",
    "        return conv_output\n",
    "\n",
    "    def maxpooling(self, image, pool_size):\n",
    "        image_height, image_width = image.shape\n",
    "\n",
    "        pooled_height = image_height // pool_size\n",
    "        pooled_width = image_width // pool_size\n",
    "\n",
    "        pooled_output = np.zeros((pooled_height, pooled_width))\n",
    "\n",
    "        for i in range(0, image_height, pool_size):\n",
    "            for j in range(0, image_width, pool_size):\n",
    "                window = image[i:i+pool_size, j:j+pool_size]\n",
    "                pooled_output[i//pool_size, j//pool_size] = np.max(window)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self, image):\n",
    "        convolution_result = self.convolutional(image)\n",
    "        pooling_result = self.maxpooling(convolution_result, pool_size=2)\n",
    "        flattened_result = self.flattening(pooling_result)\n",
    "        return flattened_result\n",
    "\n",
    "    def backward(self, image, label, learning_rate=0.001):\n",
    "        flattened_result = self.forward(image)\n",
    "\n",
    "        dweights_hidden3_output = np.zeros_like(self.weights_hidden3_output)\n",
    "        dbias_output = np.zeros_like(self.bias_output)\n",
    "\n",
    "        scores = np.dot(flattened_result, self.weights_hidden3_output) + self.bias_output\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        probs[0, label] -= 1\n",
    "        dscores = probs / len(image)\n",
    "\n",
    "        dweights_hidden3_output += np.dot(flattened_result.reshape(1, -1).T, dscores)\n",
    "        dbias_output += np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "        self.weights_hidden3_output -= learning_rate * dweights_hidden3_output\n",
    "        self.bias_output -= learning_rate * dbias_output\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def xavier_initialization(self, input_size, output_size):\n",
    "            return np.random.randn(input_size, output_size) * np.sqrt(2 / (input_size + output_size))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('/content/drive/MyDrive/oh/fashion-mnist_train.csv')\n",
    "test_data = pd.read_csv('/content/drive/MyDrive/oh/fashion-mnist_test.csv')\n",
    "\n",
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Ejemplo de visualizacion de 5 imagenes del conjunto train\n",
    "num_examples_to_show = 5\n",
    "random_indices = random.sample(range(len(X_train)), num_examples_to_show)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(1, num_examples_to_show, i + 1)\n",
    "    plt.imshow(X_train[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[idx]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Inicializacion de parametros para la creacion de la red\n",
    "hidden_sizes = [128, 64, 32]\n",
    "kernel_size = 3\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "\n",
    "conv_net = ConvNet(input_size, hidden_sizes, output_size, kernel_size)\n",
    "\n",
    "weights_input_hidden = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "bias_hidden = np.zeros(hidden_size)\n",
    "weights_hidden_output = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "bias_output = np.zeros(output_size)\n",
    "\n",
    "# Momentum para Adam\n",
    "m_input_hidden = np.zeros_like(weights_input_hidden)\n",
    "v_input_hidden = np.zeros_like(weights_input_hidden)\n",
    "m_hidden_output = np.zeros_like(weights_hidden_output)\n",
    "v_hidden_output = np.zeros_like(weights_hidden_output)\n",
    "\n",
    "beta1 = 0.9  \n",
    "beta2 = 0.999  \n",
    "epsilon = 1e-8 \n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 300\n",
    "\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Feedforward\n",
    "    hidden_layer_input = np.dot(X_train, weights_input_hidden) + bias_hidden\n",
    "    hidden_layer_output = np.maximum(0, hidden_layer_input)  # Activacion ReLU\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
    "\n",
    "    exp_scores = np.exp(output_layer_input)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    correct_logprobs = -np.log(probs[range(len(X_train)), y_train]) # Entropia cruzada\n",
    "    data_loss = np.sum(correct_logprobs) / len(X_train)\n",
    "    loss_history.append(data_loss)\n",
    "\n",
    "    # Backpropagation\n",
    "    dscores = probs\n",
    "    dscores[range(len(X_train)), y_train] -= 1\n",
    "    dscores /= len(X_train)\n",
    "\n",
    "    dweights_hidden_output = np.dot(hidden_layer_output.T, dscores)\n",
    "    dbias_output = np.sum(dscores, axis=0, keepdims=True)\n",
    "    \n",
    "    dhidden = np.dot(dscores, weights_hidden_output.T)\n",
    "    dhidden[hidden_layer_output <= 0] = 0  # Derivada de la funcion ReLU en backpropagation\n",
    "    dweights_input_hidden = np.dot(X_train.T, dhidden)\n",
    "    dbias_hidden = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "    m_input_hidden = beta1 * m_input_hidden + (1 - beta1) * dweights_input_hidden\n",
    "    v_input_hidden = beta2 * v_input_hidden + (1 - beta2) * (dweights_input_hidden ** 2)\n",
    "\n",
    "    m_hidden_output = beta1 * m_hidden_output + (1 - beta1) * dweights_hidden_output\n",
    "    v_hidden_output = beta2 * v_hidden_output + (1 - beta2) * (dweights_hidden_output ** 2)\n",
    "\n",
    "    # Correccion de Adam\n",
    "    m_input_hidden_corrected = m_input_hidden / (1 - beta1 ** (epoch + 1))\n",
    "    v_input_hidden_corrected = v_input_hidden / (1 - beta2 ** (epoch + 1))\n",
    "\n",
    "    m_hidden_output_corrected = m_hidden_output / (1 - beta1 ** (epoch + 1))\n",
    "    v_hidden_output_corrected = v_hidden_output / (1 - beta2 ** (epoch + 1))\n",
    "\n",
    "    weights_input_hidden -= learning_rate * m_input_hidden_corrected / (np.sqrt(v_input_hidden_corrected) + epsilon)\n",
    "    bias_hidden -= learning_rate * dbias_hidden.squeeze()\n",
    "\n",
    "    weights_hidden_output -= learning_rate * m_hidden_output_corrected / (np.sqrt(v_hidden_output_corrected) + epsilon)\n",
    "    bias_output -= learning_rate * dbias_output.squeeze()\n",
    "\n",
    "    hidden_layer = np.maximum(0, np.dot(X_test, weights_input_hidden) + bias_hidden)\n",
    "    scores = np.dot(hidden_layer, weights_hidden_output) + bias_output\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    test_accuracy = np.mean(predicted_class == y_test)\n",
    "    accuracy_history.append(test_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}. Loss: {data_loss:.4f}. Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "hidden_layer = np.maximum(0, np.dot(X_test, weights_input_hidden) + bias_hidden)\n",
    "scores = np.dot(hidden_layer, weights_hidden_output) + bias_output\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "\n",
    "accuracy = np.mean(predicted_class == y_test)\n",
    "print(f\"\\nPrecisión en el conjunto de prueba: {accuracy:.4f}\")\n",
    "\n",
    "# A partir de aqui es tan solo la muestra de matriz confusion, funcion perdida y funcion precision\n",
    "predictions = predicted_class.ravel()\n",
    "confusion = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = range(10) \n",
    "plt.xticks(tick_marks, range(10), rotation=45)\n",
    "plt.yticks(tick_marks, range(10))\n",
    "\n",
    "for i in range(confusion.shape[0]):\n",
    "    for j in range(confusion.shape[1]):\n",
    "        plt.text(j, i, str(confusion[i, j]), ha='center', va='center', color='white')\n",
    "\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.title('Matriz de Confusión')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(len(loss_history)), loss_history, label='Pérdida', color='red')\n",
    "plt.title('Gráfico de Pérdida')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(len(accuracy_history)), accuracy_history, label='Precisión', color='blue')\n",
    "plt.title('Gráfico de Precisión')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
